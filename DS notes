Tree

1. Binary search tree
- can be O(n) if unbalanced
- other than that, it's O(log n) meaning that you don't traverse through the whole structure, you simply decide once on each level(depth)

**Divide and conquer : simply means that we don't look at every single node, instead we divide up and make decision on each node to go left or right
(* greedy : simple, intuitive algorithm that'sused in optimization problems. This algorithm makes the optimal choice at each step as it attempts to find the overall optimal way to solve the entire problem, tps://brilliant.org/wiki/greedy-algorithm/#)
Balancing our tree
- in production we want balanced tree like AVL tree and Red black tree that automatically rebalances itself
- we won't have to implement this in interview, just mention it and tell why it's useful

resources:
https://www.cs.usfca.edu/~galles/visualization/AVLtree.html
https://medium.com/basecs/the-little-avl-tree-that-could-86a3cae410c7
https://www.cs.usfca.edu/~galles/visualization/RedBlack.html
https://medium.com/basecs/painting-nodes-black-with-red-black-trees-60eacb2be9a5

2. Heap(Binary Heap) - remember one thing, parent is always greater than children
lookup O(n) - searching is slow
insert O(log N)
delete O(log N)

lookup is O(n) because Heap is less ordered than BST where left and right had meaning(in Heap, left and right can be any value)
- Can be used anywhere where ordering is important, IT'S GREAT FOR COMPARATIVE OPERATION
- Binary Heap(only two children to a node)
- Max/Min Heap (root node has the largest/smallest value)

Binary Heap simply inserts from left to right

Binary Heap is useful for things like Priority Queuevvvvv
Q. What is Priority Queue?
A. It's not just a simple FIFO, every element has priorities and elements with higher priority is served first. Simply imagine airplane seats.
Since we know the order we just go from top to bottom and left to right and insert bunch of items. Like this, most of the time, insertion is really fast except for some cases where you get high priority item being inserted in the heap, then you have to re-balance it.

3. Trie(unlike binary tree it can have multiple children)
- specialized tree used in searching most often with texts(it tells you if a word or part of a word exists in a text)
*Prefix tree: when you search something on google, it tries and completes what you were searching(auto-suggestion)
-> benefits are time and space

search
O(length of the word)
- also good in space complexity(no duplicate keys) -> for storing n,o,t and n,e,w,s you need only one n


Graphs
- used a lot for real world modling
- has Nodes(Vertex) and Edges
- Tree is a type of Graph as well

- Directed & Undirected Graph
- Weighted and Unweighted Graph
- Cyclic, Acyclic Graph(vertices connected in a circular fashion): cyclic graph works well with weighted graphs because normally there's a way to get back to the vertex you left from.

three ways to represent graph
1. Edge List: simply a list of all the edges
2. Adjacency List: for each vertex, we store a list of which other vertices it's connected to
3. Adjacency Matrix: for each vertex, we store a list(length of all the vertices) that shows weight or simply 0 or 1 indicating whether it's connected to that specific vertex or not

Algorithm: recursion

1. find the base case
2. find the recursive case
3. get closer and closer to the answer
-> Anyway it's exponential time(2^n) why would I ever use this? -> you can optimize it with dp(memoization) to O(n)

*Anything you do with a recursion CAN be done iteratively
You can keep your code DRY(Do not Repeat Yourself), recursion could be better for readability, but it's not always the best approach(because it creates extra memory footprint-stack)

Andrei use recursion when he's working with ds that he doesn't know how deep they are.

Tail Call Optimization(it's in many languagues)
- in js(es6), you can do recursion without increasing the call stack
https://2ality.com/2015/06/tail-call-optimization.html

Recursion is useful in BFS + DFS(Searching) and in some cases in Sorting

**Rule: Every time you are using a tree or converting Something into a tree, consider recursion.
Divide and Conquer using Recursion
1. Divide a problem into a number of subproblems that are smaller instances of the same problem
2. Each instance of the subproblem is identical in nature
3. The solutions of each subproblem can be combined to solve the problem at hand

https://stackoverflow.com/questions/105838/real-world-examples-of-recursion


Sorting

elementary(something you would think of instantly in your head): Bubble, selection, insertion
- worst case O(n^2), but space complexity O(1) - no extra space needed
- note that insertion sort is really fast(O(n)) for small dataset

divide and conquer: merge, quick
- O(nlogn)
- you can implement merge sort with recursion
- quick sort use pivoting technique to make smaller lists

quick sort space complexity is better(O(log(n)) than merge sort(O(n))
time complexity however can be O(n^2) at it's worst when pivot is the smallest or the largest -> you're not splitting the list in half
*quick sort is generally the fast sorting method, but if you can't guarantee pivot being smart then you shouldn't use it.

Which sorting algorithm is the best?
1. insertion sort should be used with small dataset(+mostly sorted) - easy to implement and uses less space
2. bubble, selection sort - only for educational purposes(not really gonna use it)
3. merge sort: divide and conquer, can always guarantee list being divided in half, but it's expensive in terms of memory(O(n))
- when memory is the concern, don't use merge sort(use quick sort instead), but if you're worried about quick sort's worst case scenario(O(n^2)), then use merge sort(for example if the dataset is massive you're worried about the worst case).
4. quick sort: space complexity is better and can be faster with time, but if you pick a bad pivot then it's gonna be really slow (O(n^2))
- you want in-memory sorting to save some space then use this
5. heap sort: on average it's slower than quick sort, but uses less space
6. radix, counting sort - faster than O(nlogn), O(nk). It's an exception that applies to numbers only(range shouldn't be too large)
- so, no decimal places

Searching
- by the way, saving data in tree form is better
To traverse tree or Graph there are two different ways (O(n)) -> because we have to visit every single node to compare
1. Breadth first search (BFS)
2. Depth first search (DFS)

BFS: move down the tree and search from ltr until you find the node or tree ends
DFS: follow one branch of the tree to the end, then go back to it's ancestor and repeat
- requires less memory

- BFS vs DFS(interview) 
https://repl.it/@aneagoie/Traversal-Quiz-1

time complexity: O(n)
BFS: Good for finding shortest path(between starting point and any other reachable nodes) but needs more memory
- if you kinda know where the node is and it's likely to be in the upper level of the tree(or graph), then use this(because it searches from the closer nodes)
DFS: better when node is on the lower level of the tree. Good for checking if the path exists, uses less memory.
- it can get slow if the tree is really really deep, and it's not good for finding shortest path

DFS has three types of ordering strategies(preOrder, inOrder, postOrder)
- uses stack ds using recursion(added to the callstack) -> space complexity of DFS is O(height of the tree)

//     9
//  4     20
//1  6  15  170
1. inOrder: from left(bottom) to right(bottom) - really useful if you have binary tree
- [1, 4, 6, 9, 15, 20, 170]
- everything is in order

2. preOrder: start from parent and grab children from ltr
- [9, 4, 1, 6, 20, 15, 170]
- good for recreating the tree

3. postOrder: search left hand side first (children come before parent), then do the same on the right side, root is the last
- [1, 6, 4, 15, 170, 20, 9]
- good for deleting, getting postfix expression of an expression tree

https://www.geeksforgeeks.org/tree-traversals-inorder-preorder-and-postorder/

Graph Traversals
- graphs are broadly used in real life(recommendation engine)

https://visualgo.net/en/dfsbfs?slide=1
BFS 
- shortest path(what's the closest to our node, most related item on Amazon, 
- uses more memory
- allows us to change graph into a tree
DFS
- good for checking if something exists
- go as deep as you can and if you hit a road block(dead end) you backtrack and find a different path

code
https://www.geeksforgeeks.org/breadth-first-search-or-bfs-for-a-graph/

*Shortest path problem
- you won't implement this, but you should still know about this to talk about it on the interview
- problem with DFS is that it thinks each path(edge) to a different node(vertex) has same weight(distance, traffic might be different)
1. Bellman-Ford
- Richard Bellman wrote about DP
- this could be better because it accomodates negative weights
- it can take a long time to run with it's complexity(not the most efficient), worst case O(n^2)
https://medium.com/basecs/finding-the-shortest-path-with-a-little-help-from-dijkstra-613149fbdc8e

2. Dijkstra
- faster and more efficient than Bellman-Ford
- can't take negative weight


